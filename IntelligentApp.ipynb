{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent App with Azure OpenAI and Neo4j\n",
    "In this notebook, let's explore how to leverage Azure OpenAI to build and consume a knowledge graph in Neo4j.\n",
    "\n",
    "This notebook parses data from a public [corpus of Resumes / Curriculum Vitae](https://github.com/florex/resume_corpus) using Azure OpenAI `gpt-3.5 turbo` model. The model will be prompted to recognise and extract entities and relationships. We will then generate Neo4j Cypher queries using them and write the data to a Neo4j database.\n",
    "We will again use a `gpt-3.5 turbo` model and prompt it to convert questions in english to Cypher - Neo4j's query language, which can be used for data retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First off, check that the Python environment you installed in the readme is running this notebook. Make sure you select the `py38` kernel in the top right of this notebook. You should see a 3.8 version when you run this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to install some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from openai==0.28) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from openai==0.28) (3.8.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp->openai==0.28) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp->openai==0.28) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain>=0.0.216 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (0.0.331)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (0.0.60)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (1.26.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from langchain>=0.0.216) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from anyio<4.0->langchain>=0.0.216) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from anyio<4.0->langchain>=0.0.216) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from anyio<4.0->langchain>=0.0.216) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.216) (2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from pydantic<3,>=1->langchain>=0.0.216) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from pydantic<3,>=1->langchain>=0.0.216) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from pydantic<3,>=1->langchain>=0.0.216) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests<3,>=2->langchain>=0.0.216) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests<3,>=2->langchain>=0.0.216) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.216) (3.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: neo4j in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (5.14.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from neo4j) (2023.3.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pydantic in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from pydantic) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from pydantic) (4.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gradio in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (4.5.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (5.1.2)\n",
      "Requirement already satisfied: fastapi in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (0.104.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (0.7.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (0.17.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (3.8.1)\n",
      "Requirement already satisfied: numpy~=1.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (1.26.1)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (3.9.10)\n",
      "Requirement already satisfied: packaging in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (2.1.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (10.1.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (2.4.2)\n",
      "Requirement already satisfied: pydub in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: requests~=2.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (2.31.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio) (4.8.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (0.24.0.post1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from gradio-client==0.7.0->gradio) (2023.10.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from gradio-client==0.7.0->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from huggingface-hub>=0.14.0->gradio) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from pydantic>=2.0->gradio) (2.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests~=2.0->gradio) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests~=2.0->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests~=2.0->gradio) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests~=2.0->gradio) (2023.7.22)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from fastapi->gradio) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: httpcore in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from httpx->gradio) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.20.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
      "Requirement already satisfied: IProgress in c:\\users\\thireshsidda\\appdata\\roaming\\python\\python39\\site-packages (0.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: six in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from IProgress) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (4.66.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: colorama in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install openai==0.28\n",
    "\n",
    "%pip install --user \"langchain>=0.0.216\"\n",
    "%pip install --user neo4j\n",
    "%pip install --user pydantic\n",
    "%pip install --user gradio\n",
    "%pip install --user IProgress\n",
    "%pip install --user tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note you will need to set your OpenAI configuration\n",
    "\n",
    "API_KEY = \"05830f7918474aed9ba728978fe92d1d\"\n",
    "\n",
    "RESOURCE_ENDPOINT = \"https://gpt-demo-openai.openai.azure.com/\"\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "\n",
    "openai.api_version = \"2023-05-15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the upcoming sections, we will extract knowledge adhering to the following schema. This is a very Simplified schema to denote a Resume. Normally, you will have Domain Experts who come up with an ideal Ontology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![schema.png](images/schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve our Extraction goal as per the schema, I am going to chain a series of prompts, each focused on only one task - to extract a specific entity. By this way, you can avoid Token limitations. Also, the quality of extraction will be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_prompt_tpl=\"\"\"From the Resume text for a job aspirant below, extract Entities strictly as instructed below\n",
    "1. First, look for the Person Entity type in the text and extract the needed information defined below:\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create new entity types that aren't mentioned below. Document must be summarized and stored inside Person entity under `description` property\n",
    "    Entity Types:\n",
    "    label:'Person',id:string,role:string,description:string //Person Node\n",
    "2. Description property should be a crisp text summary and MUST NOT be more than 100 characters\n",
    "3. If you cannot find any information on the entities & relationships above, it is okay to return empty value. DO NOT create fictious data\n",
    "4. Do NOT create duplicate entities\n",
    "5. Restrict yourself to extract only Person information. No Position, Company, Education or Skill information should be focussed.\n",
    "6. NEVER Impute missing values\n",
    "Example Output JSON:\n",
    "{\"entities\": [{\"label\":\"Person\",\"id\":\"person1\",\"role\":\"Prompt Developer\",\"description\":\"Prompt Developer with more than 30 years of LLM experience\"}]}\n",
    "\n",
    "Question: Now, extract the Person for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_prompt_tpl=\"\"\"From the Resume text for a job aspirant below, extract Entities & relationships strictly as instructed below\n",
    "1. First, look for Position & Company types in the text and extract information in comma-separated format. Position Entity denotes the Person's previous or current job. Company node is the Company where they held that position.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create new entity types that aren't mentioned below. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Types:\n",
    "    label:'Position',id:string,title:string,location:string,startDate:string,endDate:string,url:string //Position Node\n",
    "    label:'Company',id:string,name:string //Company Node\n",
    "2. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. NEVER create new Relationship types that aren't mentioned below:\n",
    "    Relationship definition:\n",
    "    position|AT_COMPANY|company //Ensure this is a string in the generated output\n",
    "3. If you cannot find any information on the entities & relationships above, it is okay to return empty value. DO NOT create fictious data\n",
    "4. Do NOT create duplicate entities. \n",
    "5. No Education or Skill information should be extracted.\n",
    "6. DO NOT MISS out any Position or Company related information\n",
    "7. NEVER Impute missing values\n",
    " Example Output JSON:\n",
    "{\"entities\": [{\"label\":\"Position\",\"id\":\"position1\",\"title\":\"Software Engineer\",\"location\":\"Singapore\",startDate:\"2021-01-01\",endDate:\"present\"},{\"label\":\"Position\",\"id\":\"position2\",\"title\":\"Senior Software Engineer\",\"location\":\"Mars\",startDate:\"2020-01-01\",endDate:\"2020-12-31\"},{label:\"Company\",id:\"company1\",name:\"Neo4j Singapore Pte Ltd\"},{\"label\":\"Company\",\"id\":\"company2\",\"name\":\"Neo4j Mars Inc\"}],\"relationships\": [\"position1|AT_COMPANY|company1\",\"position2|AT_COMPANY|company2\"]}\n",
    "\n",
    "Question: Now, extract entities & relationships as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_prompt_tpl=\"\"\"From the Resume text below, extract Entities strictly as instructed below\n",
    "1. Look for prominent Skill Entities in the text. The`id` property of each entity must be alphanumeric and must be unique among the entities. NEVER create new entity types that aren't mentioned below:\n",
    "    Entity Definition:\n",
    "    label:'Skill',id:string,name:string,level:string //Skill Node\n",
    "2. NEVER Impute missing values\n",
    "3. If you do not find any level information: assume it as `expert` if the experience in that skill is more than 5 years, `intermediate` for 2-5 years and `beginner` otherwise.\n",
    "Example Output Format:\n",
    "{\"entities\": [{\"label\":\"Skill\",\"id\":\"skill1\",\"name\":\"Neo4j\",\"level\":\"expert\"},{\"label\":\"Skill\",\"id\":\"skill2\",\"name\":\"Pytorch\",\"level\":\"expert\"}]}\n",
    "\n",
    "Question: Now, extract entities as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_prompt_tpl=\"\"\"From the Resume text for a job aspirant below, extract Entities strictly as instructed below\n",
    "1. Look for Education entity type and generate the information defined below:\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create other entity types that aren't mentioned below. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Definition:\n",
    "    label:'Education',id:string,degree:string,university:string,graduationDate:string,score:string,url:string //Education Node\n",
    "2. If you cannot find any information on the entities above, it is okay to return empty value. DO NOT create fictious data\n",
    "3. Do NOT create duplicate entities or properties\n",
    "4. Strictly extract only Education. No Skill or other Entities should be extracted\n",
    "5. DO NOT MISS out any Education related entity\n",
    "6. NEVER Impute missing values\n",
    "Output JSON (Strict):\n",
    "{\"entities\": [{\"label\":\"Education\",\"id\":\"education1\",\"degree\":\"Bachelor of Science\",\"graduationDate\":\"May 2022\",\"score\":\"0.0\"}]}\n",
    "\n",
    "Question: Now, extract Education information as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to talk to the LLM with our prompt and text input. We will use the `gpt-3.5 turbo` base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_text_model(\n",
    "        deployment_id: str,\n",
    "        prompt: str,\n",
    "        max_tokens: int,\n",
    "        temperature: float,\n",
    "        ):\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        deployment_id = deployment_id,\n",
    "        prompt = prompt,\n",
    "        max_tokens = max_tokens,\n",
    "        temperature= temperature, \n",
    "        \n",
    "        )\n",
    "    return response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_relationships(prompt):\n",
    "    try:\n",
    "        res = run_text_model(deployment_id=\"gpt3-davinci3\", prompt=prompt,max_tokens=1024,temperature=0 )\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our extraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+',' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import json\n",
    "\n",
    "sample_que = \"\"\"Developer <span class=\"hl\">Developer</span> Developer - TATA CONSULTANTCY SERVICE Batavia, OH Relevant course work† Database Systems, Database Administration, Database Security & Auditing, Computer Security,Computer Networks, Programming & Software Development, IT, Information Security Concept & Admin,† IT System Acquisition & Integration, Advanced Web Development, and Ethical Hacking: Network Security & Pen Testing. Work Experience Developer TATA CONSULTANTCY SERVICE June 2016 to Present MRM (Government of ME, RI, MS) Developer†††† Working with various technologies such as Java, JSP, JSF, DB2(SQL), LDAP, BIRT report, Jazz version control, Squirrel SQL client, Hibernate, CSS, Linux, and Windows. Work as part of a team that provide support to enterprise applications. Perform miscellaneous support activities as requested by Management. Perform in-depth research and identify sources of production issues.†† SPLUNK Developer† Supporting the Splunk Operational environment for Business Solutions Unit aiming to support overall business infrastructure. Developing Splunk Queries to generate the report, monitoring, and analyzing machine generated big data for server that has been using for onsite and offshore team. Working with Splunk' premium apps such as ITSI, creating services, KPI, and glass tables. Developing app with custom dashboard with front- end ability and advanced XML to serve Business Solution unit' needs. Had in-house app presented at Splunk's .Conf Conference (2016). Help planning, prioritizing and executing development activities. Developer ( front end) intern TOMORROW PICTURES INC - Atlanta, GA April 2015 to January 2016 Assist web development team with multiple front end web technologies and involved in web technologies such as Node.js, express, json, gulp.js, jade, sass, html5, css3, bootstrap, WordPress.†Testing (manually), version control (GitHub), mock up design and ideas Education MASTER OF SCIENCE IN INFORMATION TECHNOLOGY in INFOTMATION TECHNOLOGY KENNESAW STATE UNIVERSITY - Kennesaw, GA August 2012 to May 2015 MASTER OF BUSINESS ADMINISTRATION in INTERNATIONAL BUSINESS AMERICAN INTER CONTINENTAL UNIVERSITY ATLANTA November 2003 to December 2005 BACHELOR OF ARTS in PUBLIC RELATIONS THE UNIVERSITY OF THAI CHAMBER OF COMMERCE - BANGKOK, TH June 1997 to May 2001 Skills Db2 (2 years), front end (2 years), Java (2 years), Linux (2 years), Splunk (2 years), SQL (3 years) Certifications/Licenses Splunk Certified Power User V6.3 August 2016 to Present CERT-112626 Splunk Certified Power User V6.x May 2017 to Present CERT-168138 Splunk Certified User V6.x May 2017 to Present CERT -181476 Driver's License Additional Information Skills† ∑††††SQL, PL/SQL, Knowledge of Data Modeling, Experience on Oracle database/RDBMS.† ∑††††††††Database experience on Oracle, DB2, SQL Sever, MongoDB, and MySQL.† ∑††††††††Knowledge of tools including Splunk, tableau, and wireshark.† ∑††††††††Knowledge of SCRUM/AGILE and WATERFALL methodologies.† ∑††††††††Web technology included: HTML5, CSS3, XML, JSON, JavaScript, node.js, NPM, GIT, express.js, jQuery, Angular, Bootstrap, and Restful API.† ∑††††††††Working Knowledge in JAVA, J2EE, and PHP.† Operating system Experience included: Windows, Mac OS, Linux (Ubuntu, Mint, Kali)††\"\"\"\n",
    "prompts = [person_prompt_tpl, position_prompt_tpl, skill_prompt_tpl, edu_prompt_tpl]\n",
    "results = {\"entities\": [], \"relationships\": []}\n",
    "\n",
    "for p in prompts:\n",
    "    _prompt = Template(p).substitute(ctext = clean_text(sample_que))\n",
    "    _extraction = extract_entities_relationships(_prompt)\n",
    "    if 'Answer:\\n' in _extraction:\n",
    "        _extraction = _extraction.split('Answer:\\n')[1]\n",
    "    if _extraction.strip() == '':\n",
    "        continue\n",
    "    try:\n",
    "        _extraction = json.loads(_extraction.replace(\"\\'\", \"'\").replace('`',''))\n",
    "    except json.JSONDecodeError:\n",
    "        # print(_extraction)\n",
    "        #Temp hack to ignore Skills cut off by token limitation\n",
    "        _extraction = _extraction[:_extraction.rfind(\"}\")+1] + ']}'\n",
    "        _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "    results[\"entities\"].extend(_extraction[\"entities\"])\n",
    "    if \"relationships\" in _extraction:\n",
    "        results[\"relationships\"].extend(_extraction[\"relationships\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_id = results[\"entities\"][0][\"id\"]\n",
    "\n",
    "for e in results[\"entities\"][1:]:\n",
    "    if e['label'] == 'Position':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_POSITION|{e['id']}\")\n",
    "    if e['label'] == 'Skill':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_SKILL|{e['id']}\")\n",
    "    if e['label'] == 'Education':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_EDUCATION|{e['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted entities & relationships will look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [{'label': 'Person',\n",
       "   'id': 'person1',\n",
       "   'role': 'Developer',\n",
       "   'description': 'Developer with more than 30 years of experience in Database Systems, Database Administration, Database Security & Auditing, Computer Security,Computer Networks, Programming & Software Development, IT, Information Security Concept & Admin, IT System Acquisition & Integration, Advanced Web Development, and Ethical Hacking: Network Security & Pen Testing.'},\n",
       "  {'label': 'Position',\n",
       "   'id': 'position1',\n",
       "   'title': 'Developer',\n",
       "   'location': 'Batavia, OH',\n",
       "   'startDate': 'June 2016',\n",
       "   'endDate': 'Present'},\n",
       "  {'label': 'Position',\n",
       "   'id': 'position2',\n",
       "   'title': 'MRM (Government of ME, RI, MS) Developer',\n",
       "   'location': 'Batavia, OH',\n",
       "   'startDate': 'June 2016',\n",
       "   'endDate': 'Present'},\n",
       "  {'label': 'Position',\n",
       "   'id': 'position3',\n",
       "   'title': 'SPLUNK Developer',\n",
       "   'location': 'Batavia, OH',\n",
       "   'startDate': 'June 2016',\n",
       "   'endDate': 'Present'},\n",
       "  {'label': 'Position',\n",
       "   'id': 'position4',\n",
       "   'title': 'Developer (front end) intern',\n",
       "   'location': 'Atlanta, GA',\n",
       "   'startDate': 'April 2015',\n",
       "   'endDate': 'January 2016'},\n",
       "  {'label': 'Company', 'id': 'company1', 'name': 'TATA CONSULTANTCY SERVICE'},\n",
       "  {'label': 'Company', 'id': 'company2', 'name': 'TOMORROW PICTURES INC'},\n",
       "  {'label': 'Skill', 'id': 'skill1', 'name': 'Db2', 'level': 'intermediate'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill2',\n",
       "   'name': 'Front End',\n",
       "   'level': 'intermediate'},\n",
       "  {'label': 'Skill', 'id': 'skill3', 'name': 'Java', 'level': 'intermediate'},\n",
       "  {'label': 'Skill', 'id': 'skill4', 'name': 'Linux', 'level': 'intermediate'},\n",
       "  {'label': 'Skill', 'id': 'skill5', 'name': 'Splunk', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill6', 'name': 'SQL', 'level': 'intermediate'},\n",
       "  {'label': 'Skill', 'id': 'skill7', 'name': 'PL/SQL', 'level': 'beginner'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill8',\n",
       "   'name': 'Data Modeling',\n",
       "   'level': 'beginner'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill9',\n",
       "   'name': 'Oracle Database/RDBMS',\n",
       "   'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill10', 'name': 'Tableau', 'level': 'beginner'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill11',\n",
       "   'name': 'Wireshark',\n",
       "   'level': 'beginner'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill12',\n",
       "   'name': 'SCRUM/AGILE',\n",
       "   'level': 'beginner'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill13',\n",
       "   'name': 'WATERFALL',\n",
       "   'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill14', 'name': 'HTML5', 'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill15', 'name': 'CSS3', 'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill16', 'name': 'XML', 'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill17', 'name': 'JSON', 'level': 'beginner'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill18',\n",
       "   'name': 'JavaScript',\n",
       "   'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill19', 'name': 'Node.js', 'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill20', 'name': 'NPM', 'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill21', 'name': 'GIT', 'level': 'beginner'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill22',\n",
       "   'name': 'Express.js',\n",
       "   'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill23', 'name': 'jQuery', 'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill24', 'name': 'Angular', 'level': 'beginner'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill25',\n",
       "   'name': 'Bootstrap',\n",
       "   'level': 'beginner'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill26',\n",
       "   'name': 'Restful API',\n",
       "   'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill27', 'name': 'JAVA', 'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill28', 'name': 'J2EE', 'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill29', 'name': 'PHP', 'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill30', 'name': 'Windows', 'level': 'beginner'},\n",
       "  {'label': 'Skill', 'id': 'skill31', 'name': 'Mac OS', 'level': 'beginner'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill32',\n",
       "   'name': 'Linux (Ubuntu, Mint, Kali)',\n",
       "   'level': 'beginner'},\n",
       "  {'label': 'Education',\n",
       "   'id': 'education1',\n",
       "   'degree': 'Bachelor of Arts',\n",
       "   'university': 'The University of Thai Chamber of Commerce',\n",
       "   'graduationDate': 'May 2001',\n",
       "   'url': 'https://www.utcc.ac.th/en/'},\n",
       "  {'label': 'Education',\n",
       "   'id': 'education2',\n",
       "   'degree': 'Master of Science in Information Technology',\n",
       "   'university': 'Kennesaw State University',\n",
       "   'graduationDate': 'May 2015',\n",
       "   'url': 'https://www.kennesaw.edu/'},\n",
       "  {'label': 'Education',\n",
       "   'id': 'education3',\n",
       "   'degree': 'Master of Business Administration',\n",
       "   'university': 'American Inter Continental University Atlanta',\n",
       "   'graduationDate': 'December 2005',\n",
       "   'url': 'https://www.aiuniv.edu/'}],\n",
       " 'relationships': ['position1|AT_COMPANY|company1',\n",
       "  'position2|AT_COMPANY|company1',\n",
       "  'position3|AT_COMPANY|company1',\n",
       "  'position4|AT_COMPANY|company2',\n",
       "  'person1|HAS_POSITION|position1',\n",
       "  'person1|HAS_POSITION|position2',\n",
       "  'person1|HAS_POSITION|position3',\n",
       "  'person1|HAS_POSITION|position4',\n",
       "  'person1|HAS_SKILL|skill1',\n",
       "  'person1|HAS_SKILL|skill2',\n",
       "  'person1|HAS_SKILL|skill3',\n",
       "  'person1|HAS_SKILL|skill4',\n",
       "  'person1|HAS_SKILL|skill5',\n",
       "  'person1|HAS_SKILL|skill6',\n",
       "  'person1|HAS_SKILL|skill7',\n",
       "  'person1|HAS_SKILL|skill8',\n",
       "  'person1|HAS_SKILL|skill9',\n",
       "  'person1|HAS_SKILL|skill10',\n",
       "  'person1|HAS_SKILL|skill11',\n",
       "  'person1|HAS_SKILL|skill12',\n",
       "  'person1|HAS_SKILL|skill13',\n",
       "  'person1|HAS_SKILL|skill14',\n",
       "  'person1|HAS_SKILL|skill15',\n",
       "  'person1|HAS_SKILL|skill16',\n",
       "  'person1|HAS_SKILL|skill17',\n",
       "  'person1|HAS_SKILL|skill18',\n",
       "  'person1|HAS_SKILL|skill19',\n",
       "  'person1|HAS_SKILL|skill20',\n",
       "  'person1|HAS_SKILL|skill21',\n",
       "  'person1|HAS_SKILL|skill22',\n",
       "  'person1|HAS_SKILL|skill23',\n",
       "  'person1|HAS_SKILL|skill24',\n",
       "  'person1|HAS_SKILL|skill25',\n",
       "  'person1|HAS_SKILL|skill26',\n",
       "  'person1|HAS_SKILL|skill27',\n",
       "  'person1|HAS_SKILL|skill28',\n",
       "  'person1|HAS_SKILL|skill29',\n",
       "  'person1|HAS_SKILL|skill30',\n",
       "  'person1|HAS_SKILL|skill31',\n",
       "  'person1|HAS_SKILL|skill32',\n",
       "  'person1|HAS_EDUCATION|education1',\n",
       "  'person1|HAS_EDUCATION|education2',\n",
       "  'person1|HAS_EDUCATION|education3']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion Cypher Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entities and relationships we got from the LLM have to be transformed to Cypher so we can write them into Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_prop_str(prop_dict, _id):\n",
    "    s = []\n",
    "    for key, val in prop_dict.items():\n",
    "        if key != 'label' and key != 'id':\n",
    "            s.append(_id+\".\"+key+' = \"'+ str(val).replace('\\\"', '\"').replace('\"', '\\\"')+'\"')\n",
    "    return ' ON CREATE SET ' + ','.join(s)\n",
    "\n",
    "\n",
    "def get_cypher_compliant_var(_id):\n",
    "    s = \"_\" + re.sub(r'[\\W_]', '', _id).lower()  #avoid numbers appearing as firstchar; replace spaces\n",
    "    return s[:20] #restrict variable size\n",
    "\n",
    "\n",
    "def generate_cypher(file_name, in_json):\n",
    "    e_map = {}\n",
    "    e_stmt = []\n",
    "    r_stmt = []\n",
    "    e_stmt_tpl = Template(\"($id:$label{id:'$key'})\")\n",
    "    r_stmt_tpl = Template(\"\"\"\n",
    "      MATCH $src\n",
    "      MATCH $tgt\n",
    "      MERGE ($src_id)-[:$rel]->($tgt_id)\"\"\")\n",
    "    \n",
    "    for obj in in_json:\n",
    "        for j in obj['entities']:\n",
    "            props = ''\n",
    "            label = j['label']\n",
    "            id = ''\n",
    "            if label == 'Person':\n",
    "                id = 'p' + str(file_name)\n",
    "            elif label == 'Position':\n",
    "                c = j['id'].replace('Position', '_')\n",
    "                id = f'j{str(file_name)}{c}'\n",
    "            elif label == 'Education':\n",
    "                c = j['id'].replace('education', '_')\n",
    "                id = f'e{str(file_name)}{c}'\n",
    "            else:\n",
    "                id = get_cypher_compliant_var(j['name'])\n",
    "            if label in ['Person', 'Position', 'Education', 'Skill', 'Company']:\n",
    "                varname = get_cypher_compliant_var(j['id'])\n",
    "                stmt = e_stmt_tpl.substitute(id=varname, label=label, key=id)\n",
    "                e_map[varname] = stmt\n",
    "                e_stmt.append('MERGE' + stmt + get_prop_str(j, varname))\n",
    "\n",
    "    \n",
    "        for st in obj['relationships']:\n",
    "            rels = st.split(\"|\")\n",
    "            src_id = get_cypher_compliant_var(rels[0].strip())\n",
    "            rel = rels[1].strip()\n",
    "            if rel in ['HAS_SKILL', 'HAS_EDUCATION', 'AT_COMPANY', 'HAS_POSITION']: #We ignore other relationships\n",
    "                tgt_id = get_cypher_compliant_var(rels[2].strip())\n",
    "                stmt = r_stmt_tpl.substitute(\n",
    "                    src_id=src_id, tgt_id=tgt_id, src=e_map[src_id], tgt=e_map[tgt_id], rel=rel)\n",
    "                r_stmt.append(stmt)\n",
    "\n",
    "    return e_stmt, r_stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MERGE(_person1:Person{id:\\'pmy_cv\\'}) ON CREATE SET _person1.role = \"Developer\",_person1.description = \"Developer with more than 30 years of experience in Database Systems, Database Administration, Database Security & Auditing, Computer Security,Computer Networks, Programming & Software Development, IT, Information Security Concept & Admin, IT System Acquisition & Integration, Advanced Web Development, and Ethical Hacking: Network Security & Pen Testing.\"', 'MERGE(_position1:Position{id:\\'jmy_cvposition1\\'}) ON CREATE SET _position1.title = \"Developer\",_position1.location = \"Batavia, OH\",_position1.startDate = \"June 2016\",_position1.endDate = \"Present\"', 'MERGE(_position2:Position{id:\\'jmy_cvposition2\\'}) ON CREATE SET _position2.title = \"MRM (Government of ME, RI, MS) Developer\",_position2.location = \"Batavia, OH\",_position2.startDate = \"June 2016\",_position2.endDate = \"Present\"', 'MERGE(_position3:Position{id:\\'jmy_cvposition3\\'}) ON CREATE SET _position3.title = \"SPLUNK Developer\",_position3.location = \"Batavia, OH\",_position3.startDate = \"June 2016\",_position3.endDate = \"Present\"', 'MERGE(_position4:Position{id:\\'jmy_cvposition4\\'}) ON CREATE SET _position4.title = \"Developer (front end) intern\",_position4.location = \"Atlanta, GA\",_position4.startDate = \"April 2015\",_position4.endDate = \"January 2016\"', 'MERGE(_company1:Company{id:\\'_tataconsultantcyser\\'}) ON CREATE SET _company1.name = \"TATA CONSULTANTCY SERVICE\"', 'MERGE(_company2:Company{id:\\'_tomorrowpicturesinc\\'}) ON CREATE SET _company2.name = \"TOMORROW PICTURES INC\"', 'MERGE(_skill1:Skill{id:\\'_db2\\'}) ON CREATE SET _skill1.name = \"Db2\",_skill1.level = \"intermediate\"', 'MERGE(_skill2:Skill{id:\\'_frontend\\'}) ON CREATE SET _skill2.name = \"Front End\",_skill2.level = \"intermediate\"', 'MERGE(_skill3:Skill{id:\\'_java\\'}) ON CREATE SET _skill3.name = \"Java\",_skill3.level = \"intermediate\"', 'MERGE(_skill4:Skill{id:\\'_linux\\'}) ON CREATE SET _skill4.name = \"Linux\",_skill4.level = \"intermediate\"', 'MERGE(_skill5:Skill{id:\\'_splunk\\'}) ON CREATE SET _skill5.name = \"Splunk\",_skill5.level = \"expert\"', 'MERGE(_skill6:Skill{id:\\'_sql\\'}) ON CREATE SET _skill6.name = \"SQL\",_skill6.level = \"intermediate\"', 'MERGE(_skill7:Skill{id:\\'_oracledatabaserdbms\\'}) ON CREATE SET _skill7.name = \"Oracle Database/RDBMS\",_skill7.level = \"intermediate\"', 'MERGE(_skill8:Skill{id:\\'_tableau\\'}) ON CREATE SET _skill8.name = \"Tableau\",_skill8.level = \"intermediate\"', 'MERGE(_skill9:Skill{id:\\'_wireshark\\'}) ON CREATE SET _skill9.name = \"Wireshark\",_skill9.level = \"intermediate\"', 'MERGE(_skill10:Skill{id:\\'_html5\\'}) ON CREATE SET _skill10.name = \"HTML5\",_skill10.level = \"intermediate\"', 'MERGE(_skill11:Skill{id:\\'_css3\\'}) ON CREATE SET _skill11.name = \"CSS3\",_skill11.level = \"intermediate\"', 'MERGE(_skill12:Skill{id:\\'_xml\\'}) ON CREATE SET _skill12.name = \"XML\",_skill12.level = \"intermediate\"', 'MERGE(_skill13:Skill{id:\\'_json\\'}) ON CREATE SET _skill13.name = \"JSON\",_skill13.level = \"intermediate\"', 'MERGE(_skill14:Skill{id:\\'_javascript\\'}) ON CREATE SET _skill14.name = \"JavaScript\",_skill14.level = \"intermediate\"', 'MERGE(_skill15:Skill{id:\\'_nodejs\\'}) ON CREATE SET _skill15.name = \"Node.js\",_skill15.level = \"intermediate\"', 'MERGE(_skill16:Skill{id:\\'_npm\\'}) ON CREATE SET _skill16.name = \"NPM\",_skill16.level = \"intermediate\"', 'MERGE(_skill17:Skill{id:\\'_git\\'}) ON CREATE SET _skill17.name = \"GIT\",_skill17.level = \"intermediate\"', 'MERGE(_skill18:Skill{id:\\'_expressjs\\'}) ON CREATE SET _skill18.name = \"Express.js\",_skill18.level = \"intermediate\"', 'MERGE(_skill19:Skill{id:\\'_jquery\\'}) ON CREATE SET _skill19.name = \"jQuery\",_skill19.level = \"intermediate\"', 'MERGE(_skill20:Skill{id:\\'_angular\\'}) ON CREATE SET _skill20.name = \"Angular\",_skill20.level = \"intermediate\"', 'MERGE(_skill21:Skill{id:\\'_bootstrap\\'}) ON CREATE SET _skill21.name = \"Bootstrap\",_skill21.level = \"intermediate\"', 'MERGE(_skill22:Skill{id:\\'_restfulapi\\'}) ON CREATE SET _skill22.name = \"Restful API\",_skill22.level = \"intermediate\"', 'MERGE(_skill23:Skill{id:\\'_java\\'}) ON CREATE SET _skill23.name = \"JAVA\",_skill23.level = \"intermediate\"', 'MERGE(_skill24:Skill{id:\\'_j2ee\\'}) ON CREATE SET _skill24.name = \"J2EE\",_skill24.level = \"intermediate\"', 'MERGE(_skill25:Skill{id:\\'_php\\'}) ON CREATE SET _skill25.name = \"PHP\",_skill25.level = \"intermediate\"', 'MERGE(_skill26:Skill{id:\\'_windows\\'}) ON CREATE SET _skill26.name = \"Windows\",_skill26.level = \"intermediate\"', 'MERGE(_skill27:Skill{id:\\'_macos\\'}) ON CREATE SET _skill27.name = \"Mac OS\",_skill27.level = \"intermediate\"', 'MERGE(_skill28:Skill{id:\\'_linuxubuntumintkali\\'}) ON CREATE SET _skill28.name = \"Linux (Ubuntu, Mint, Kali)\",_skill28.level = \"intermediate\"', 'MERGE(_education1:Education{id:\\'emy_cv_1\\'}) ON CREATE SET _education1.degree = \"Bachelor of Arts\",_education1.university = \"The University of Thai Chamber of Commerce\",_education1.graduationDate = \"May 2001\",_education1.url = \"https://www.utcc.ac.th/en/\"', 'MERGE(_education2:Education{id:\\'emy_cv_2\\'}) ON CREATE SET _education2.degree = \"Master of Science in Information Technology\",_education2.university = \"Kennesaw State University\",_education2.graduationDate = \"May 2015\",_education2.url = \"https://www.kennesaw.edu/\"', 'MERGE(_education3:Education{id:\\'emy_cv_3\\'}) ON CREATE SET _education3.degree = \"Master of Business Administration\",_education3.university = \"American Inter Continental University Atlanta\",_education3.graduationDate = \"December 2005\",_education3.url = \"https://www.aiuniv.edu/\"'] [\"\\n      MATCH (_position1:Position{id:'jmy_cvposition1'})\\n      MATCH (_company1:Company{id:'_tataconsultantcyser'})\\n      MERGE (_position1)-[:AT_COMPANY]->(_company1)\", \"\\n      MATCH (_position2:Position{id:'jmy_cvposition2'})\\n      MATCH (_company1:Company{id:'_tataconsultantcyser'})\\n      MERGE (_position2)-[:AT_COMPANY]->(_company1)\", \"\\n      MATCH (_position3:Position{id:'jmy_cvposition3'})\\n      MATCH (_company1:Company{id:'_tataconsultantcyser'})\\n      MERGE (_position3)-[:AT_COMPANY]->(_company1)\", \"\\n      MATCH (_position4:Position{id:'jmy_cvposition4'})\\n      MATCH (_company2:Company{id:'_tomorrowpicturesinc'})\\n      MERGE (_position4)-[:AT_COMPANY]->(_company2)\"]\n"
     ]
    }
   ],
   "source": [
    "ent_cyp, rel_cyp = generate_cypher('my_cv', [results])\n",
    "\n",
    "print(ent_cyp, rel_cyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need a Neo4j Graph Database resource. You can deploy that on Azure cloud Marketplace [here](https://azuremarketplace.microsoft.com/).\n",
    "Or else youcan directly install Neo4j Desktop on your local system.\n",
    "\n",
    "With that complete, you'll need to install the Neo4j library and set up your database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "\n",
    "# connectionUrl = \"bolt://localhost:7687\"  #input(\"Neo4j Conection URL\")\n",
    "# username = \"neo4j\"                          #input(\"DB Username\")\n",
    "# password = getpass.getpass(\"Thireshsidda963i7\") #input(\"DB Password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(uri=\"bolt://localhost:7687\", auth=(\"neo4j\", \"Thireshsidda963i7\"))\n",
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def run_query(query, params={}):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, params)\n",
    "        return pd.DataFrame([r.values() for r in result], columns = result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the data, create constraints as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_query('CREATE CONSTRAINT unique_person_id IF NOT EXISTS FOR (n:Person) REQUIRE (n.id) IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_position_id IF NOT EXISTS FOR (n:Position) REQUIRE (n.id) IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_skill_id IF NOT EXISTS FOR (n:Skill) REQUIRE n.id IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_education_id IF NOT EXISTS FOR (n:Education) REQUIRE n.id IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_company_id IF NOT EXISTS FOR (n:Company) REQUIRE n.id IS UNIQUE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 898 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for e in ent_cyp:\n",
    "    run_query(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting the relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 32.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for r in rel_cyp:\n",
    "    run_query(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your ingested data from the above commands might look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ingested_data.png](images/ingested_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we have a plenty of Resumes in our Data directory. Let us run a pipeline to ingest only a few of them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from string import Template\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from timeit import default_timer as Timer\n",
    "def run_extraction(f, text):\n",
    "    start = Timer()\n",
    "    prompts = [person_prompt_tpl, position_prompt_tpl, skill_prompt_tpl, edu_prompt_tpl]\n",
    "    results = {\"entities\": [], \"relationships\": []}\n",
    "    for p in prompts:\n",
    "        _prompt = Template(p). substitute(ctext=text)\n",
    "        _extraction = extract_entities_relationships(_prompt)\n",
    "        if 'Answer:\\n' in _extraction:\n",
    "            _extraction = _extraction.split('Answer:\\n ')[1]\n",
    "        if _extraction.strip() == '':\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "        except json.JSONDecodeError:\n",
    "            #Temp hack to ignore Skills cut off by token limitation\n",
    "            _extraction = _extraction[:_extraction.rfind(\"}\")+1] + ']}'\n",
    "            _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "        results[\"entities\"].extend(_extraction[\"entities\"])\n",
    "        if \"relationships\" in _extraction:\n",
    "            results[\"relationships\"].extend(_extraction[\"relationships\"])\n",
    "    person_id= results[\"entities\"][0][\"id\"]\n",
    "    for e in results[\"entities\"][1:]:\n",
    "        if e['label'] == 'Position':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_POSITION|{e['id']}\")\n",
    "        if e['label'] == 'Skill':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_SKILL|{e['id']}\")\n",
    "        if e['label'] == 'Education':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_EDUCATION|{e['id']}\")\n",
    "        \n",
    "    end = Timer()\n",
    "    elapsed = (end-start)\n",
    "    print(f\"   {f} Entity Extraction took {elapsed}secs\")\n",
    "    return [results]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_pipeline(files):\n",
    "    failed_files = []\n",
    "    i = 0\n",
    "\n",
    "    for f in files:\n",
    "        i += 1 \n",
    "        try:\n",
    "            with open(f, 'r', encoding=\"utf8\", errors = 'ignore') as file:\n",
    "                print(f\" {f}: Reading file No. ({i})\")\n",
    "                data = file.read().rstrip()\n",
    "                text = data\n",
    "                print(f\" {f}: Extracting Entities & Relationships\")\n",
    "                results = run_extraction(f,text)\n",
    "\n",
    "                print(f\"  {f}: Generating Cypher\")\n",
    "                ent_cyp, rel_cyp = generate_cypher(Path(f).stem, results)\n",
    "                \n",
    "                print(f\"   {f}: Ingesting Entities\")\n",
    "                for e in ent_cyp:\n",
    "                    run_query(e)\n",
    "\n",
    "                print(f\"  {f}: Ingesting Relationships\")\n",
    "                for r in rel_cyp:\n",
    "                    run_query(r)\n",
    "                print(f\"   {f}: Processing DONE\")\n",
    "        except Exception as e:\n",
    "            print(f\" {f}: Processing Failed with exception {e}\")\n",
    "            failed_files.append(f)\n",
    "    return failed_files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_pipeline(start=0, count=1):\n",
    "    txt_files = glob.glob(\"data/*.txt\")[start:count]\n",
    "    print(f\"Running pipeline for {len(txt_files)} files\")\n",
    "    failed_files = process_pipeline(txt_files)\n",
    "    print(failed_files)\n",
    "    return failed_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the pipeline only for the first 50 files. This will only process those 10 files and imgested them to Neo4J. It usually takes around 20-25 minutes for 100 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%%time\n",
    "failed_files = run_pipeline(0, 50)   #Run the ingestion pipeline for filesfrom index 0 to 100  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If processing failed for some files due to API Rate limit, you can retry as below. For token limitation error, it is better to chunk the text and retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline for 50 files\n",
      " data\\05499.txt: Reading file No. (1)\n",
      " data\\05499.txt: Extracting Entities & Relationships\n",
      " data\\05499.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 106 (char 105)\n",
      " data\\05500.txt: Reading file No. (2)\n",
      " data\\05500.txt: Extracting Entities & Relationships\n",
      " data\\05500.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 111 (char 110)\n",
      " data\\05501.txt: Reading file No. (3)\n",
      " data\\05501.txt: Extracting Entities & Relationships\n",
      "   data\\05501.txt Entity Extraction took 29.073927599999934secs\n",
      "  data\\05501.txt: Generating Cypher\n",
      "   data\\05501.txt: Ingesting Entities\n",
      "  data\\05501.txt: Ingesting Relationships\n",
      "   data\\05501.txt: Processing DONE\n",
      " data\\05502.txt: Reading file No. (4)\n",
      " data\\05502.txt: Extracting Entities & Relationships\n",
      "   data\\05502.txt Entity Extraction took 46.02420159999997secs\n",
      "  data\\05502.txt: Generating Cypher\n",
      "   data\\05502.txt: Ingesting Entities\n",
      "  data\\05502.txt: Ingesting Relationships\n",
      "   data\\05502.txt: Processing DONE\n",
      " data\\05503.txt: Reading file No. (5)\n",
      " data\\05503.txt: Extracting Entities & Relationships\n",
      "   data\\05503.txt Entity Extraction took 46.5227582secs\n",
      "  data\\05503.txt: Generating Cypher\n",
      " data\\05503.txt: Processing Failed with exception '_null'\n",
      " data\\05504.txt: Reading file No. (6)\n",
      " data\\05504.txt: Extracting Entities & Relationships\n",
      "   data\\05504.txt Entity Extraction took 23.342199800000003secs\n",
      "  data\\05504.txt: Generating Cypher\n",
      "   data\\05504.txt: Ingesting Entities\n",
      "  data\\05504.txt: Ingesting Relationships\n",
      "   data\\05504.txt: Processing DONE\n",
      " data\\05505.txt: Reading file No. (7)\n",
      " data\\05505.txt: Extracting Entities & Relationships\n",
      "   data\\05505.txt Entity Extraction took 29.167793800000027secs\n",
      "  data\\05505.txt: Generating Cypher\n",
      "   data\\05505.txt: Ingesting Entities\n",
      "  data\\05505.txt: Ingesting Relationships\n",
      "   data\\05505.txt: Processing DONE\n",
      " data\\05506.txt: Reading file No. (8)\n",
      " data\\05506.txt: Extracting Entities & Relationships\n",
      "   data\\05506.txt Entity Extraction took 36.70362439999997secs\n",
      "  data\\05506.txt: Generating Cypher\n",
      "   data\\05506.txt: Ingesting Entities\n",
      "  data\\05506.txt: Ingesting Relationships\n",
      "   data\\05506.txt: Processing DONE\n",
      " data\\05507.txt: Reading file No. (9)\n",
      " data\\05507.txt: Extracting Entities & Relationships\n",
      "   data\\05507.txt Entity Extraction took 36.64803660000007secs\n",
      "  data\\05507.txt: Generating Cypher\n",
      "   data\\05507.txt: Ingesting Entities\n",
      "  data\\05507.txt: Ingesting Relationships\n",
      "   data\\05507.txt: Processing DONE\n",
      " data\\05508.txt: Reading file No. (10)\n",
      " data\\05508.txt: Extracting Entities & Relationships\n",
      "   data\\05508.txt Entity Extraction took 26.74662950000004secs\n",
      "  data\\05508.txt: Generating Cypher\n",
      "   data\\05508.txt: Ingesting Entities\n",
      "  data\\05508.txt: Ingesting Relationships\n",
      "   data\\05508.txt: Processing DONE\n",
      " data\\05509.txt: Reading file No. (11)\n",
      " data\\05509.txt: Extracting Entities & Relationships\n",
      " data\\05509.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 98 (char 97)\n",
      " data\\05510.txt: Reading file No. (12)\n",
      " data\\05510.txt: Extracting Entities & Relationships\n",
      "   data\\05510.txt Entity Extraction took 16.779668799999968secs\n",
      "  data\\05510.txt: Generating Cypher\n",
      "   data\\05510.txt: Ingesting Entities\n",
      "  data\\05510.txt: Ingesting Relationships\n",
      "   data\\05510.txt: Processing DONE\n",
      " data\\05511.txt: Reading file No. (13)\n",
      " data\\05511.txt: Extracting Entities & Relationships\n",
      "   data\\05511.txt Entity Extraction took 20.98554820000004secs\n",
      "  data\\05511.txt: Generating Cypher\n",
      "   data\\05511.txt: Ingesting Entities\n",
      "  data\\05511.txt: Ingesting Relationships\n",
      "   data\\05511.txt: Processing DONE\n",
      " data\\05512.txt: Reading file No. (14)\n",
      " data\\05512.txt: Extracting Entities & Relationships\n",
      "   data\\05512.txt Entity Extraction took 19.036360599999966secs\n",
      "  data\\05512.txt: Generating Cypher\n",
      "   data\\05512.txt: Ingesting Entities\n",
      "  data\\05512.txt: Ingesting Relationships\n",
      "   data\\05512.txt: Processing DONE\n",
      " data\\05513.txt: Reading file No. (15)\n",
      " data\\05513.txt: Extracting Entities & Relationships\n",
      "   data\\05513.txt Entity Extraction took 27.693386499999974secs\n",
      "  data\\05513.txt: Generating Cypher\n",
      "   data\\05513.txt: Ingesting Entities\n",
      "  data\\05513.txt: Ingesting Relationships\n",
      "   data\\05513.txt: Processing DONE\n",
      " data\\05514.txt: Reading file No. (16)\n",
      " data\\05514.txt: Extracting Entities & Relationships\n",
      "   data\\05514.txt Entity Extraction took 19.179331799999886secs\n",
      "  data\\05514.txt: Generating Cypher\n",
      "   data\\05514.txt: Ingesting Entities\n",
      "  data\\05514.txt: Ingesting Relationships\n",
      "   data\\05514.txt: Processing DONE\n",
      " data\\05515.txt: Reading file No. (17)\n",
      " data\\05515.txt: Extracting Entities & Relationships\n",
      " data\\05515.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 108 (char 107)\n",
      " data\\05516.txt: Reading file No. (18)\n",
      " data\\05516.txt: Extracting Entities & Relationships\n",
      "   data\\05516.txt Entity Extraction took 24.159448699999984secs\n",
      "  data\\05516.txt: Generating Cypher\n",
      "   data\\05516.txt: Ingesting Entities\n",
      "  data\\05516.txt: Ingesting Relationships\n",
      "   data\\05516.txt: Processing DONE\n",
      " data\\05517.txt: Reading file No. (19)\n",
      " data\\05517.txt: Extracting Entities & Relationships\n",
      "   data\\05517.txt Entity Extraction took 23.45771250000007secs\n",
      "  data\\05517.txt: Generating Cypher\n",
      "   data\\05517.txt: Ingesting Entities\n",
      "  data\\05517.txt: Ingesting Relationships\n",
      "   data\\05517.txt: Processing DONE\n",
      " data\\05518.txt: Reading file No. (20)\n",
      " data\\05518.txt: Extracting Entities & Relationships\n",
      "   data\\05518.txt Entity Extraction took 13.323817499999905secs\n",
      "  data\\05518.txt: Generating Cypher\n",
      "   data\\05518.txt: Ingesting Entities\n",
      "  data\\05518.txt: Ingesting Relationships\n",
      "   data\\05518.txt: Processing DONE\n",
      " data\\05519.txt: Reading file No. (21)\n",
      " data\\05519.txt: Extracting Entities & Relationships\n",
      "   data\\05519.txt Entity Extraction took 17.665837799999963secs\n",
      "  data\\05519.txt: Generating Cypher\n",
      "   data\\05519.txt: Ingesting Entities\n",
      "  data\\05519.txt: Ingesting Relationships\n",
      "   data\\05519.txt: Processing DONE\n",
      " data\\05520.txt: Reading file No. (22)\n",
      " data\\05520.txt: Extracting Entities & Relationships\n",
      "   data\\05520.txt Entity Extraction took 31.31539879999991secs\n",
      "  data\\05520.txt: Generating Cypher\n",
      "   data\\05520.txt: Ingesting Entities\n",
      "  data\\05520.txt: Ingesting Relationships\n",
      "   data\\05520.txt: Processing DONE\n",
      " data\\05521.txt: Reading file No. (23)\n",
      " data\\05521.txt: Extracting Entities & Relationships\n",
      " data\\05521.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 101 (char 100)\n",
      " data\\05522.txt: Reading file No. (24)\n",
      " data\\05522.txt: Extracting Entities & Relationships\n",
      " data\\05522.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 105 (char 104)\n",
      " data\\05523.txt: Reading file No. (25)\n",
      " data\\05523.txt: Extracting Entities & Relationships\n",
      "   data\\05523.txt Entity Extraction took 30.180212700000084secs\n",
      "  data\\05523.txt: Generating Cypher\n",
      "   data\\05523.txt: Ingesting Entities\n",
      "  data\\05523.txt: Ingesting Relationships\n",
      "   data\\05523.txt: Processing DONE\n",
      " data\\05524.txt: Reading file No. (26)\n",
      " data\\05524.txt: Extracting Entities & Relationships\n",
      "   data\\05524.txt Entity Extraction took 46.99513430000002secs\n",
      "  data\\05524.txt: Generating Cypher\n",
      "   data\\05524.txt: Ingesting Entities\n",
      "  data\\05524.txt: Ingesting Relationships\n",
      "   data\\05524.txt: Processing DONE\n",
      " data\\05525.txt: Reading file No. (27)\n",
      " data\\05525.txt: Extracting Entities & Relationships\n",
      " data\\05525.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 107 (char 106)\n",
      " data\\05526.txt: Reading file No. (28)\n",
      " data\\05526.txt: Extracting Entities & Relationships\n",
      " data\\05526.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 104 (char 103)\n",
      " data\\05527.txt: Reading file No. (29)\n",
      " data\\05527.txt: Extracting Entities & Relationships\n",
      "   data\\05527.txt Entity Extraction took 6.464720199999874secs\n",
      "  data\\05527.txt: Generating Cypher\n",
      "   data\\05527.txt: Ingesting Entities\n",
      "  data\\05527.txt: Ingesting Relationships\n",
      "   data\\05527.txt: Processing DONE\n",
      " data\\05528.txt: Reading file No. (30)\n",
      " data\\05528.txt: Extracting Entities & Relationships\n",
      "   data\\05528.txt Entity Extraction took 28.69269680000002secs\n",
      "  data\\05528.txt: Generating Cypher\n",
      "   data\\05528.txt: Ingesting Entities\n",
      "  data\\05528.txt: Ingesting Relationships\n",
      "   data\\05528.txt: Processing DONE\n",
      " data\\05529.txt: Reading file No. (31)\n",
      " data\\05529.txt: Extracting Entities & Relationships\n",
      "   data\\05529.txt Entity Extraction took 13.904054200000019secs\n",
      "  data\\05529.txt: Generating Cypher\n",
      "   data\\05529.txt: Ingesting Entities\n",
      "  data\\05529.txt: Ingesting Relationships\n",
      "   data\\05529.txt: Processing DONE\n",
      " data\\05530.txt: Reading file No. (32)\n",
      " data\\05530.txt: Extracting Entities & Relationships\n",
      "   data\\05530.txt Entity Extraction took 15.31474789999993secs\n",
      "  data\\05530.txt: Generating Cypher\n",
      "   data\\05530.txt: Ingesting Entities\n",
      "  data\\05530.txt: Ingesting Relationships\n",
      "   data\\05530.txt: Processing DONE\n",
      " data\\05531.txt: Reading file No. (33)\n",
      " data\\05531.txt: Extracting Entities & Relationships\n",
      "   data\\05531.txt Entity Extraction took 23.193524099999877secs\n",
      "  data\\05531.txt: Generating Cypher\n",
      "   data\\05531.txt: Ingesting Entities\n",
      "  data\\05531.txt: Ingesting Relationships\n",
      "   data\\05531.txt: Processing DONE\n",
      " data\\05532.txt: Reading file No. (34)\n",
      " data\\05532.txt: Extracting Entities & Relationships\n",
      "   data\\05532.txt Entity Extraction took 17.660274699999945secs\n",
      "  data\\05532.txt: Generating Cypher\n",
      "   data\\05532.txt: Ingesting Entities\n",
      "  data\\05532.txt: Ingesting Relationships\n",
      "   data\\05532.txt: Processing DONE\n",
      " data\\05533.txt: Reading file No. (35)\n",
      " data\\05533.txt: Extracting Entities & Relationships\n",
      "   data\\05533.txt Entity Extraction took 24.642976699999963secs\n",
      "  data\\05533.txt: Generating Cypher\n",
      " data\\05533.txt: Processing Failed with exception '_position3'\n",
      " data\\05534.txt: Reading file No. (36)\n",
      " data\\05534.txt: Extracting Entities & Relationships\n",
      "This model's maximum context length is 4097 tokens, however you requested 4202 tokens (3178 in your prompt; 1024 for the completion). Please reduce your prompt; or completion length.\n",
      " data\\05534.txt: Processing Failed with exception argument of type 'NoneType' is not iterable\n",
      " data\\05535.txt: Reading file No. (37)\n",
      " data\\05535.txt: Extracting Entities & Relationships\n",
      " data\\05535.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 100 (char 99)\n",
      " data\\05536.txt: Reading file No. (38)\n",
      " data\\05536.txt: Extracting Entities & Relationships\n",
      " data\\05536.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 103 (char 102)\n",
      " data\\05537.txt: Reading file No. (39)\n",
      " data\\05537.txt: Extracting Entities & Relationships\n",
      " data\\05537.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 105 (char 104)\n",
      " data\\05538.txt: Reading file No. (40)\n",
      " data\\05538.txt: Extracting Entities & Relationships\n",
      "   data\\05538.txt Entity Extraction took 39.31098199999997secs\n",
      "  data\\05538.txt: Generating Cypher\n",
      "   data\\05538.txt: Ingesting Entities\n",
      "  data\\05538.txt: Ingesting Relationships\n",
      "   data\\05538.txt: Processing DONE\n",
      " data\\05539.txt: Reading file No. (41)\n",
      " data\\05539.txt: Extracting Entities & Relationships\n",
      " data\\05539.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 106 (char 105)\n",
      " data\\05540.txt: Reading file No. (42)\n",
      " data\\05540.txt: Extracting Entities & Relationships\n",
      "This model's maximum context length is 4097 tokens, however you requested 4492 tokens (3468 in your prompt; 1024 for the completion). Please reduce your prompt; or completion length.\n",
      " data\\05540.txt: Processing Failed with exception argument of type 'NoneType' is not iterable\n",
      " data\\05541.txt: Reading file No. (43)\n",
      " data\\05541.txt: Extracting Entities & Relationships\n",
      "   data\\05541.txt Entity Extraction took 25.607723999999962secs\n",
      "  data\\05541.txt: Generating Cypher\n",
      "   data\\05541.txt: Ingesting Entities\n",
      "  data\\05541.txt: Ingesting Relationships\n",
      "   data\\05541.txt: Processing DONE\n",
      " data\\05542.txt: Reading file No. (44)\n",
      " data\\05542.txt: Extracting Entities & Relationships\n",
      "This model's maximum context length is 4097 tokens, however you requested 4149 tokens (3125 in your prompt; 1024 for the completion). Please reduce your prompt; or completion length.\n",
      " data\\05542.txt: Processing Failed with exception argument of type 'NoneType' is not iterable\n",
      " data\\05543.txt: Reading file No. (45)\n",
      " data\\05543.txt: Extracting Entities & Relationships\n",
      "   data\\05543.txt Entity Extraction took 31.45102750000001secs\n",
      "  data\\05543.txt: Generating Cypher\n",
      "   data\\05543.txt: Ingesting Entities\n",
      "  data\\05543.txt: Ingesting Relationships\n",
      "   data\\05543.txt: Processing DONE\n",
      " data\\05544.txt: Reading file No. (46)\n",
      " data\\05544.txt: Extracting Entities & Relationships\n",
      "   data\\05544.txt Entity Extraction took 54.13385549999998secs\n",
      "  data\\05544.txt: Generating Cypher\n",
      "   data\\05544.txt: Ingesting Entities\n",
      "  data\\05544.txt: Ingesting Relationships\n",
      "   data\\05544.txt: Processing DONE\n",
      " data\\05545.txt: Reading file No. (47)\n",
      " data\\05545.txt: Extracting Entities & Relationships\n",
      "   data\\05545.txt Entity Extraction took 33.50385789999996secs\n",
      "  data\\05545.txt: Generating Cypher\n",
      "   data\\05545.txt: Ingesting Entities\n",
      "  data\\05545.txt: Ingesting Relationships\n",
      "   data\\05545.txt: Processing DONE\n",
      " data\\05546.txt: Reading file No. (48)\n",
      " data\\05546.txt: Extracting Entities & Relationships\n",
      "   data\\05546.txt Entity Extraction took 33.815647000000126secs\n",
      "  data\\05546.txt: Generating Cypher\n",
      "   data\\05546.txt: Ingesting Entities\n",
      "  data\\05546.txt: Ingesting Relationships\n",
      "   data\\05546.txt: Processing DONE\n",
      " data\\05547.txt: Reading file No. (49)\n",
      " data\\05547.txt: Extracting Entities & Relationships\n",
      "   data\\05547.txt Entity Extraction took 48.4169480999999secs\n",
      "  data\\05547.txt: Generating Cypher\n",
      "   data\\05547.txt: Ingesting Entities\n",
      "  data\\05547.txt: Ingesting Relationships\n",
      "   data\\05547.txt: Processing DONE\n",
      " data\\05548.txt: Reading file No. (50)\n",
      " data\\05548.txt: Extracting Entities & Relationships\n",
      "This model's maximum context length is 4097 tokens, however you requested 4620 tokens (3596 in your prompt; 1024 for the completion). Please reduce your prompt; or completion length.\n",
      " data\\05548.txt: Processing Failed with exception argument of type 'NoneType' is not iterable\n",
      "['data\\\\05499.txt', 'data\\\\05500.txt', 'data\\\\05503.txt', 'data\\\\05509.txt', 'data\\\\05515.txt', 'data\\\\05521.txt', 'data\\\\05522.txt', 'data\\\\05525.txt', 'data\\\\05526.txt', 'data\\\\05533.txt', 'data\\\\05534.txt', 'data\\\\05535.txt', 'data\\\\05536.txt', 'data\\\\05537.txt', 'data\\\\05539.txt', 'data\\\\05540.txt', 'data\\\\05542.txt', 'data\\\\05548.txt']\n",
      "CPU times: total: 1.06 s\n",
      "Wall time: 18min 41s\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data\\05499.txt: Reading file No. (1)\n",
      " data\\05499.txt: Extracting Entities & Relationships\n",
      " data\\05499.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 106 (char 105)\n",
      " data\\05500.txt: Reading file No. (2)\n",
      " data\\05500.txt: Extracting Entities & Relationships\n",
      " data\\05500.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 111 (char 110)\n",
      " data\\05503.txt: Reading file No. (3)\n",
      " data\\05503.txt: Extracting Entities & Relationships\n",
      "   data\\05503.txt Entity Extraction took 39.99196359999996secs\n",
      "  data\\05503.txt: Generating Cypher\n",
      " data\\05503.txt: Processing Failed with exception '_null'\n",
      " data\\05509.txt: Reading file No. (4)\n",
      " data\\05509.txt: Extracting Entities & Relationships\n",
      " data\\05509.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 98 (char 97)\n",
      " data\\05515.txt: Reading file No. (5)\n",
      " data\\05515.txt: Extracting Entities & Relationships\n",
      " data\\05515.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 108 (char 107)\n",
      " data\\05521.txt: Reading file No. (6)\n",
      " data\\05521.txt: Extracting Entities & Relationships\n",
      " data\\05521.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 101 (char 100)\n",
      " data\\05522.txt: Reading file No. (7)\n",
      " data\\05522.txt: Extracting Entities & Relationships\n",
      " data\\05522.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 105 (char 104)\n",
      " data\\05525.txt: Reading file No. (8)\n",
      " data\\05525.txt: Extracting Entities & Relationships\n",
      " data\\05525.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 107 (char 106)\n",
      " data\\05526.txt: Reading file No. (9)\n",
      " data\\05526.txt: Extracting Entities & Relationships\n",
      " data\\05526.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 104 (char 103)\n",
      " data\\05533.txt: Reading file No. (10)\n",
      " data\\05533.txt: Extracting Entities & Relationships\n",
      "   data\\05533.txt Entity Extraction took 25.623988300000065secs\n",
      "  data\\05533.txt: Generating Cypher\n",
      " data\\05533.txt: Processing Failed with exception '_position3'\n",
      " data\\05534.txt: Reading file No. (11)\n",
      " data\\05534.txt: Extracting Entities & Relationships\n",
      "This model's maximum context length is 4097 tokens, however you requested 4202 tokens (3178 in your prompt; 1024 for the completion). Please reduce your prompt; or completion length.\n",
      " data\\05534.txt: Processing Failed with exception argument of type 'NoneType' is not iterable\n",
      " data\\05535.txt: Reading file No. (12)\n",
      " data\\05535.txt: Extracting Entities & Relationships\n",
      " data\\05535.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 100 (char 99)\n",
      " data\\05536.txt: Reading file No. (13)\n",
      " data\\05536.txt: Extracting Entities & Relationships\n",
      " data\\05536.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 103 (char 102)\n",
      " data\\05537.txt: Reading file No. (14)\n",
      " data\\05537.txt: Extracting Entities & Relationships\n",
      " data\\05537.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 105 (char 104)\n",
      " data\\05539.txt: Reading file No. (15)\n",
      " data\\05539.txt: Extracting Entities & Relationships\n",
      " data\\05539.txt: Processing Failed with exception Expecting property name enclosed in double quotes: line 1 column 106 (char 105)\n",
      " data\\05540.txt: Reading file No. (16)\n",
      " data\\05540.txt: Extracting Entities & Relationships\n",
      "This model's maximum context length is 4097 tokens, however you requested 4492 tokens (3468 in your prompt; 1024 for the completion). Please reduce your prompt; or completion length.\n",
      " data\\05540.txt: Processing Failed with exception argument of type 'NoneType' is not iterable\n",
      " data\\05542.txt: Reading file No. (17)\n",
      " data\\05542.txt: Extracting Entities & Relationships\n",
      "This model's maximum context length is 4097 tokens, however you requested 4149 tokens (3125 in your prompt; 1024 for the completion). Please reduce your prompt; or completion length.\n",
      " data\\05542.txt: Processing Failed with exception argument of type 'NoneType' is not iterable\n",
      " data\\05548.txt: Reading file No. (18)\n",
      " data\\05548.txt: Extracting Entities & Relationships\n",
      "This model's maximum context length is 4097 tokens, however you requested 4620 tokens (3596 in your prompt; 1024 for the completion). Please reduce your prompt; or completion length.\n",
      " data\\05548.txt: Processing Failed with exception argument of type 'NoneType' is not iterable\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 2min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data\\\\05499.txt',\n",
       " 'data\\\\05500.txt',\n",
       " 'data\\\\05503.txt',\n",
       " 'data\\\\05509.txt',\n",
       " 'data\\\\05515.txt',\n",
       " 'data\\\\05521.txt',\n",
       " 'data\\\\05522.txt',\n",
       " 'data\\\\05525.txt',\n",
       " 'data\\\\05526.txt',\n",
       " 'data\\\\05533.txt',\n",
       " 'data\\\\05534.txt',\n",
       " 'data\\\\05535.txt',\n",
       " 'data\\\\05536.txt',\n",
       " 'data\\\\05537.txt',\n",
       " 'data\\\\05539.txt',\n",
       " 'data\\\\05540.txt',\n",
       " 'data\\\\05542.txt',\n",
       " 'data\\\\05548.txt']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "failed_files = process_pipeline(failed_files)\n",
    "failed_files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cypher Geneation for Consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the model to generate Cypher (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code family of models perform well for Cypher generation with few-shot prompting. However,they are not tunable at the moment. If you need to tune model for a specific Cypher Generation task, you can consider 'gpt-3.5 turbo' model we usd during the ingestion process above. So, the tuning section below is completely optional.\n",
    "\n",
    "The 'gpt-3.5 turbo' base model can be tuned to generate more accurate Cypher. Lets see how to adapter tune it. We will try to tune the model with some Cypher statements. The model achieves some Cypher geneation capability but could be better. It is suggested to try with at least a few hundred statements. You should aim  for more quality training data.\n",
    "\n",
    "The total training time below takes more than an hour. The tuned adapter model is going to stay within your tenant and your training data will not be used to train the base model which is frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us upload our training set in `jsonl` format to a Azure BLOB. We will use this file `finetuning/eng-to-cypher-trng.jsonl` for our fine-tuning. You can take a look over the data there.\n",
    "\n",
    "OpenAI expects you to adhere to this format for each line of the `jsonl` file. \n",
    "```json\n",
    "{\"input_text\": \"MY_INPUT_PROMPT\", \"output_text\": \"CYPHER_QUERY\"} \n",
    "```\n",
    "\n",
    "To get about Fine tuning Azure OpenAI service, explore this ['Azure OpenAI Fine tuning tutorial](https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/fine-tune?tabs=command-line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you got some changes in the training data, ensure that you upload the updated file in a different name than your previous tuning exercises. Because OpenAI caches data uploaded previously, it skips any file validation and uses the previously uploaded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning an Azure OpenAI Language Model (LLM) for Cypher Queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preparation:\n",
    "Assemble a training dataset containing pairs of user queries and their corresponding Cypher queries in a suitable format.\n",
    "\n",
    "2. Data Formatting for Training:\n",
    "Structure the training data into a format compatible with Azure OpenAI's fine-tuning requirements. This could be in JSONL format with input and output text fields.\n",
    "\n",
    "3. Token Count Analysis:\n",
    "Analyze the token counts of your training and validation data to ensure they fit within the Azure OpenAI model's token limits.\n",
    "\n",
    "4. Upload Fine-tuning Files:\n",
    "Use Azure OpenAI's SDK to upload the training and validation dataset files.\n",
    "\n",
    "5. Initiate Fine-tuning:\n",
    "Trigger the fine-tuning process by creating a fine-tuning job using the Azure OpenAI SDK.\n",
    "\n",
    "6. Track Training Job Status:\n",
    "Continuously monitor the status of the fine-tuning job until it completes.\n",
    "\n",
    "7. Deploy the Fine-tuned Model:\n",
    "Deploy the fine-tuned model using the Azure OpenAI REST API. Provide necessary parameters such as the subscription ID, resource group, model deployment name, and fine-tuned model ID.\n",
    "\n",
    "8. Use the Deployed Model:\n",
    "Employ the deployed fine-tuned model to generate Cypher queries based on user input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1: Data Preparation\n",
    "\n",
    "In this initial step, we've compiled 120 examples in the eng-cyp-training.jsonl file. Each example consists of a structured format containing an \"input_text\" representing the user's query and an \"output_text\" field containing the corresponding Cypher query. These examples have been crafted to cover a range of questions and their expected Cypher query responses relevant to our application's context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: Data Formatting for Training and Validation\n",
    "\n",
    "Following the Data Preparation phase, the data was formatted to align with Azure OpenAI's fine-tuning requirements. Each example was transformed into a specific structure, leveraging the \"messages\" format, where \"input_text\" serves as the user's query and \"output_text\" represents the assistant's response, formatted as Cypher queries. The total dataset was split into two distinct subsets: 60 examples for training and another 60 for validation, ensuring a balanced representation across both sets.\n",
    "\n",
    "This formatted dataset will serve as the foundation for fine-tuning our Azure OpenAI Language Model (LLM) to accurately generate Cypher queries based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted data for training and validation is ready!\n"
     ]
    }
   ],
   "source": [
    "# Data Formatting for Training and Validation\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# File paths\n",
    "input_file_path = r'C:\\Users\\ThireshSidda\\Desktop\\GraphAIbot\\finetuning\\eng-to-cypher-trng.jsonl'\n",
    "output_train_file_path = 'formatted_train_data.jsonl'\n",
    "output_validation_file_path = 'formatted_validation_data.jsonl'\n",
    "\n",
    "# Read the data from the input file\n",
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    examples = [json.loads(line) for line in file]\n",
    "\n",
    "# Split examples into training and validation sets (60 each)\n",
    "training_examples = examples[:60]\n",
    "validation_examples = examples[60:120]\n",
    "\n",
    "# Function to format examples\n",
    "def format_example(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"input_text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"output_text\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Format training examples\n",
    "formatted_training_data = [format_example(example) for example in training_examples]\n",
    "\n",
    "# Format validation examples\n",
    "formatted_validation_data = [format_example(example) for example in validation_examples]\n",
    "\n",
    "# Write formatted data to separate files for training and validation\n",
    "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
    "    for example in formatted_training_data:\n",
    "        train_file.write(json.dumps(example) + '\\n')\n",
    "\n",
    "with open(output_validation_file_path, 'w', encoding='utf-8') as validation_file:\n",
    "    for example in formatted_validation_data:\n",
    "        validation_file.write(json.dumps(example) + '\\n')\n",
    "\n",
    "print(\"Formatted data for training and validation is ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Token Count Analysis\n",
    "\n",
    "This step involves analyzing the token counts within the training and validation data to ensure compatibility with Azure OpenAI's model token limits. The code computes token counts for each message and assistant message, providing statistical insights into the token distributions within these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\thireshsidda\\myenv\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Libraries needed to install for third step.\n",
    "%pip install requests tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file formatted_train_data.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Distribution of total tokens:\n",
      "min / max: 646, 714\n",
      "mean / median: 677.8, 673.5\n",
      "p5 / p95: 660.0, 700.3\n",
      "\n",
      "#### Distribution of assisitant tokens:\n",
      "min / max: 22, 82\n",
      "mean / median: 52.53333333333333, 49.0\n",
      "p5 / p95: 38.0, 73.1\n",
      "**************************************************\n",
      "Processing file formatted_validation_data.jsonl\n",
      "\n",
      "#### Distribution of total tokens:\n",
      "min / max: 670, 801\n",
      "mean / median: 719.4333333333333, 714.0\n",
      "p5 / p95: 697.0, 750.0\n",
      "\n",
      "#### Distribution of assisitant tokens:\n",
      "min / max: 45, 146\n",
      "mean / median: 84.55, 83.0\n",
      "p5 / p95: 68.6, 106.1\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Setting up encoding\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\") # default encoding used by gpt-4, turbo, and text-embedding-ada-002 models\n",
    "\n",
    "def num_tokens_from_messages(messages, tokens_per_message = 3, tokens_per_name = 1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "files = ['formatted_train_data.jsonl', 'formatted_validation_data.jsonl']\n",
    "for file in files:\n",
    "    print(f\"Processing file {file}\")\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f] \n",
    "\n",
    "    total_tokens = []\n",
    "    assisitant_tokens = []\n",
    "\n",
    "    for ex in dataset:\n",
    "        messages = ex.get(\"messages\", {})\n",
    "        total_tokens.append(num_tokens_from_messages(messages))\n",
    "        assisitant_tokens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "    print_distribution(total_tokens, \"total tokens\")\n",
    "    print_distribution(assisitant_tokens, \"assisitant tokens\")\n",
    "    print('*' * 50)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Upload Fine-tuning Files\n",
    "\n",
    "To upload the training and validation datasets to Azure OpenAI, we use the provided SDK. This code snippet does the following:\n",
    "\n",
    "1.Authentication Setup: Sets up the necessary API credentials for Azure OpenAI.\n",
    "\n",
    "2.File Definitions: Defines the file names for the training and validation datasets.\n",
    "\n",
    "3.Upload Files: Utilizes the Azure OpenAI SDK to upload the training and validation dataset files.\n",
    "\n",
    "4.Display IDs: Prints the IDs assigned to the uploaded training and validation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID:  file-1b331e8887d84ff99e04fe9ad6296e41\n",
      "Validation file ID:  file-21cdd56581454e72bb12e4b106458ba7\n"
     ]
    }
   ],
   "source": [
    "# Upload fine-tuning files\n",
    "\n",
    "# import openai\n",
    "# import os\n",
    "\n",
    "# openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\") \n",
    "# openai.api_base =  os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# openai.api_type = 'azure'\n",
    "# openai.api_version = '2023-09-15-preview'    This API version or later is required to access fine-tuning for turbo/babbage-002/davinci-002\n",
    "\n",
    "training_file_name = \"formatted_train_data.jsonl\"\n",
    "validation_file_name = \"formatted_validation_data.jsonl\"\n",
    "\n",
    "\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI with the SDK.\n",
    "training_response = openai.File.create(file=open(training_file_name, \"rb\"), purpose=\"fine-tune\", user_provided_filename=\"formatted_train_data.jsonl\")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "\n",
    "validation_response = openai.File.create(file=open(validation_file_name, \"rb\"), purpose=\"fine-tune\", user_provided_filename=\"formatted_validation_data.jsonl\")\n",
    "validation_file_id = validation_response[\"id\"]\n",
    "\n",
    "\n",
    "print(\"Training file ID: \", training_file_id)\n",
    "print(\"Validation file ID: \", validation_file_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = openai.File.list()\n",
    "\n",
    "models = openai.Model.list()\n",
    "\n",
    "#finetune_models = openai.FineTuningJob.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"statistics\": {\n",
      "        \"tokens\": 404,\n",
      "        \"examples\": 10\n",
      "      },\n",
      "      \"bytes\": 2704,\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"training_set.jsonl\",\n",
      "      \"id\": \"file-76b5b2211bfe4c03a69119da9bf9d819\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1698928221,\n",
      "      \"updated_at\": 1698928224,\n",
      "      \"object\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"statistics\": {\n",
      "        \"tokens\": 401,\n",
      "        \"examples\": 10\n",
      "      },\n",
      "      \"bytes\": 2640,\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"validation_set.jsonl\",\n",
      "      \"id\": \"file-0eac9614869948de93f234b76aa18e24\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1698928222,\n",
      "      \"updated_at\": 1698928224,\n",
      "      \"object\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"statistics\": {\n",
      "        \"examples\": 0\n",
      "      },\n",
      "      \"error\": {\n",
      "        \"code\": \"jsonlValidationFailed\",\n",
      "        \"message\": \"Validation of jsonl file failed: Invalid json content in line 1. Each line for chat completion must contain at least one non-empty message for role 'assistant'\"\n",
      "      },\n",
      "      \"bytes\": 66391,\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"formatted_train_data.jsonl\",\n",
      "      \"id\": \"file-7379b114dcde4a41b04c2333de408c8b\",\n",
      "      \"status\": \"failed\",\n",
      "      \"created_at\": 1700741699,\n",
      "      \"updated_at\": 1700741700,\n",
      "      \"object\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"statistics\": {\n",
      "        \"examples\": 0\n",
      "      },\n",
      "      \"error\": {\n",
      "        \"code\": \"jsonlValidationFailed\",\n",
      "        \"message\": \"Validation of jsonl file failed: Invalid json content in line 1. Each line for chat completion must contain at least one non-empty message for role 'assistant'\"\n",
      "      },\n",
      "      \"bytes\": 66391,\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"formatted_train_data.jsonl\",\n",
      "      \"id\": \"file-4bc9e691a8a74c7b8583ae3e628b1926\",\n",
      "      \"status\": \"failed\",\n",
      "      \"created_at\": 1700741912,\n",
      "      \"updated_at\": 1700741917,\n",
      "      \"object\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"statistics\": {\n",
      "        \"examples\": 0\n",
      "      },\n",
      "      \"error\": {\n",
      "        \"code\": \"jsonlValidationFailed\",\n",
      "        \"message\": \"Validation of jsonl file failed: Invalid json content in line 1. Each line for chat completion must contain at least one non-empty message for role 'assistant'\"\n",
      "      },\n",
      "      \"bytes\": 75301,\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"formatted_validation_data.jsonl\",\n",
      "      \"id\": \"file-0e470609faff4bbe8124f0db6170b01c\",\n",
      "      \"status\": \"failed\",\n",
      "      \"created_at\": 1700741913,\n",
      "      \"updated_at\": 1700741917,\n",
      "      \"object\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"statistics\": {\n",
      "        \"examples\": 0\n",
      "      },\n",
      "      \"error\": {\n",
      "        \"code\": \"jsonlValidationFailed\",\n",
      "        \"message\": \"Validation of jsonl file failed: Invalid json content in line 1. Each line for chat completion must contain at least one non-empty message for role 'assistant'\"\n",
      "      },\n",
      "      \"bytes\": 66391,\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"formatted_train_data.jsonl\",\n",
      "      \"id\": \"file-32832729ff3147b8b5ea0eac62b2199a\",\n",
      "      \"status\": \"failed\",\n",
      "      \"created_at\": 1700744121,\n",
      "      \"updated_at\": 1700744125,\n",
      "      \"object\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"statistics\": {\n",
      "        \"examples\": 0\n",
      "      },\n",
      "      \"error\": {\n",
      "        \"code\": \"jsonlValidationFailed\",\n",
      "        \"message\": \"Validation of jsonl file failed: Invalid json content in line 1. Each line for chat completion must contain at least one non-empty message for role 'assistant'\"\n",
      "      },\n",
      "      \"bytes\": 75301,\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"formatted_validation_data.jsonl\",\n",
      "      \"id\": \"file-d08ab954afd94412be70bb94ba94198c\",\n",
      "      \"status\": \"failed\",\n",
      "      \"created_at\": 1700744123,\n",
      "      \"updated_at\": 1700744125,\n",
      "      \"object\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"statistics\": {\n",
      "        \"tokens\": 48255,\n",
      "        \"examples\": 60\n",
      "      },\n",
      "      \"bytes\": 164374,\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"formatted_train_data.jsonl\",\n",
      "      \"id\": \"file-1a1fee1d1f9d450f9491c66df5b3c049\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1700753713,\n",
      "      \"updated_at\": 1700753715,\n",
      "      \"object\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"statistics\": {\n",
      "        \"tokens\": 51419,\n",
      "        \"examples\": 60\n",
      "      },\n",
      "      \"bytes\": 173284,\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"formatted_validation_data.jsonl\",\n",
      "      \"id\": \"file-d0c3b031922c425bb02d85752a433719\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1700753714,\n",
      "      \"updated_at\": 1700753715,\n",
      "      \"object\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"statistics\": {\n",
      "        \"tokens\": 48255,\n",
      "        \"examples\": 60\n",
      "      },\n",
      "      \"bytes\": 164374,\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"formatted_train_data.jsonl\",\n",
      "      \"id\": \"file-1b331e8887d84ff99e04fe9ad6296e41\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1700757027,\n",
      "      \"updated_at\": 1700757031,\n",
      "      \"object\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"statistics\": {\n",
      "        \"tokens\": 51419,\n",
      "        \"examples\": 60\n",
      "      },\n",
      "      \"bytes\": 173284,\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"formatted_validation_data.jsonl\",\n",
      "      \"id\": \"file-21cdd56581454e72bb12e4b106458ba7\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1700757032,\n",
      "      \"updated_at\": 1700757037,\n",
      "      \"object\": \"file\"\n",
      "    }\n",
      "  ],\n",
      "  \"object\": \"list\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": false,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"ada\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1646092800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": false,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"babbage\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1646092800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": false,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"curie\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1646092800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"preview\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1721001600\n",
      "      },\n",
      "      \"id\": \"dall-e-3-3.0\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1691712000,\n",
      "      \"updated_at\": 1691712000,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": false,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"davinci\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1646092800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-ada-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1646092800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-babbage-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1646092800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-curie-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1646092800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-davinci-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1646092800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-davinci-002\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1642809600,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": true,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"gpt-35-turbo-0301\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1678320000,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": true,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1711843200,\n",
      "        \"inference\": 1711843200\n",
      "      },\n",
      "      \"id\": \"gpt-35-turbo-0613\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1687132800,\n",
      "      \"updated_at\": 1687132800,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1727654400\n",
      "      },\n",
      "      \"id\": \"gpt-35-turbo-instruct-0914\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1694649600,\n",
      "      \"updated_at\": 1694649600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": true,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1711843200\n",
      "      },\n",
      "      \"id\": \"gpt-35-turbo-16k-0613\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1687132800,\n",
      "      \"updated_at\": 1687132800,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": true,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"gpt-4-0314\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1679356800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": true,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1735862400,\n",
      "        \"inference\": 1711843200\n",
      "      },\n",
      "      \"id\": \"gpt-4-0613\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1687132800,\n",
      "      \"updated_at\": 1687132800,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": true,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"gpt-4-32k-0314\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1679356800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": true,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1711843200\n",
      "      },\n",
      "      \"id\": \"gpt-4-32k-0613\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1687132800,\n",
      "      \"updated_at\": 1687132800,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-davinci-003\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1664496000,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": false,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"preview\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-davinci-fine-tune-002\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1659398400,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"code-cushman-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1642809600,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": false,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"preview\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"code-cushman-fine-tune-002\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1677456000,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"preview\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"code-davinci-002\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1657497600,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": false,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"fine_tune\": 1720137600,\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"code-davinci-fine-tune-002\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1662508800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-similarity-ada-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-similarity-babbage-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-similarity-curie-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-similarity-davinci-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-search-ada-doc-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-search-ada-query-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-search-babbage-doc-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-search-babbage-query-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-search-curie-doc-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-search-curie-query-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-search-davinci-doc-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"text-search-davinci-query-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"code-search-ada-code-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"code-search-ada-text-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"code-search-babbage-code-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"code-search-babbage-text-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1653004800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1738454400\n",
      "      },\n",
      "      \"id\": \"text-embedding-ada-002\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1675296000,\n",
      "      \"updated_at\": 1675296000,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1743638400\n",
      "      },\n",
      "      \"id\": \"text-embedding-ada-002-2\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1680480000,\n",
      "      \"updated_at\": 1680480000,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"preview\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1721001600\n",
      "      },\n",
      "      \"id\": \"dall-e-3\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1691712000,\n",
      "      \"updated_at\": 1691712000,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": true,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"gpt-35-turbo\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1678320000,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": true,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1727654400\n",
      "      },\n",
      "      \"id\": \"gpt-35-turbo-instruct\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1694649600,\n",
      "      \"updated_at\": 1694649600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": true,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1711843200\n",
      "      },\n",
      "      \"id\": \"gpt-35-turbo-16k\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1687132800,\n",
      "      \"updated_at\": 1687132800,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": true,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"gpt-4\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1679356800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": true,\n",
      "        \"embeddings\": false\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1720137600\n",
      "      },\n",
      "      \"id\": \"gpt-4-32k\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1679356800,\n",
      "      \"updated_at\": 1688601600,\n",
      "      \"object\": \"model\"\n",
      "    },\n",
      "    {\n",
      "      \"capabilities\": {\n",
      "        \"fine_tune\": false,\n",
      "        \"inference\": true,\n",
      "        \"completion\": false,\n",
      "        \"chat_completion\": false,\n",
      "        \"embeddings\": true\n",
      "      },\n",
      "      \"lifecycle_status\": \"generally-available\",\n",
      "      \"deprecation\": {\n",
      "        \"inference\": 1743638400\n",
      "      },\n",
      "      \"id\": \"text-embedding-ada-002\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1680480000,\n",
      "      \"updated_at\": 1680480000,\n",
      "      \"object\": \"model\"\n",
      "    }\n",
      "  ],\n",
      "  \"object\": \"list\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Initiate Fine-tuning\n",
    "\n",
    "Initiating the fine-tuning process involves the following steps:\n",
    "\n",
    "1.Fine-tuning Job Creation: Uses the Azure OpenAI SDK to create a fine-tuning job by specifying the training and validation file IDs along with the desired model for fine-tuning.\n",
    "\n",
    "2.Job ID Retrieval and Monitoring: Retrieves the job ID and status to monitor the progress of the fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Resource not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ThireshSidda\\Desktop\\GraphAIbot\\graphAI.ipynb Cell 67\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ThireshSidda/Desktop/GraphAIbot/graphAI.ipynb#Y132sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mFineTuningJob\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ThireshSidda/Desktop/GraphAIbot/graphAI.ipynb#Y132sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     training_file\u001b[39m=\u001b[39;49mtraining_file_id,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ThireshSidda/Desktop/GraphAIbot/graphAI.ipynb#Y132sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     validation_file\u001b[39m=\u001b[39;49mvalidation_file_id,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ThireshSidda/Desktop/GraphAIbot/graphAI.ipynb#Y132sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-35-turbo-0613\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ThireshSidda/Desktop/GraphAIbot/graphAI.ipynb#Y132sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ThireshSidda/Desktop/GraphAIbot/graphAI.ipynb#Y132sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m job_id \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ThireshSidda/Desktop/GraphAIbot/graphAI.ipynb#Y132sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# You can use the job ID to monitor the status of the fine-tuning job.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ThireshSidda/Desktop/GraphAIbot/graphAI.ipynb#Y132sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# The fine-tuning job will take some time to start and complete.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ThireshSidda\\myenv\\lib\\site-packages\\openai\\api_resources\\abstract\\createable_api_resource.py:57\u001b[0m, in \u001b[0;36mCreateableAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m     40\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m     48\u001b[0m ):\n\u001b[0;32m     49\u001b[0m     requestor, url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__prepare_create_requestor(\n\u001b[0;32m     50\u001b[0m         api_key,\n\u001b[0;32m     51\u001b[0m         api_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m         organization,\n\u001b[0;32m     55\u001b[0m     )\n\u001b[1;32m---> 57\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m     58\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params, request_id\u001b[39m=\u001b[39;49mrequest_id\n\u001b[0;32m     59\u001b[0m     )\n\u001b[0;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39mconvert_to_openai_object(\n\u001b[0;32m     62\u001b[0m         response,\n\u001b[0;32m     63\u001b[0m         api_key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         plain_old_data\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mplain_old_data,\n\u001b[0;32m     67\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ThireshSidda\\myenv\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\ThireshSidda\\myenv\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ThireshSidda\\myenv\\lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: Resource not found"
     ]
    }
   ],
   "source": [
    "response = openai.FineTuningJob.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=\"gpt-35-turbo-0613\",\n",
    ")\n",
    "\n",
    "job_id = response[\"id\"]\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tuning job.\n",
    "# The fine-tuning job will take some time to start and complete.\n",
    "\n",
    "print(\"Job ID:\", response[\"id\"])\n",
    "print(\"Status:\", response[\"status\"])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talent Finder Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a chatbot that can help our interaction with Neo4j using English.\n",
    "\n",
    "Both Azure OpenAI and Neo4j support Langchain. We will be using Langchain to quickly build a chatbot that converts English to Cypher and then executes it on Neo4j. This is augmented using generative AI before sending the response to the user. This makes graph consumption easier for non-Cypher experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to create Neo4j and OpenAI LLM Connection objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.llms import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"You are an expert Neo4j Cypher translator who understands the question in english and convert to Cypher strictly based on the Neo4j Schema provided and following the instructions below:\n",
    "1. Generate Cypher query compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE keywords in the cypher. Use alias when using the WITH keyword\n",
    "3. Use only Nodes and relationships mentioned in the schema\n",
    "4. Always enclose the Cypher output inside 3 backticks\n",
    "5. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Company name use `toLower(c.name) contains 'neo4j'`\n",
    "6. Candidate node is synonymous to Person\n",
    "7. Always use aliases to refer the node in the query\n",
    "8. Cypher is NOT SQL. So, do not mix and match the syntaxes\n",
    "Schema:\n",
    "{schema}\n",
    "Samples:\n",
    "Question: How many expert java developers attend more than one universities?\n",
    "Answer: MATCH (p:Person)-[:HAS_SKILL]->(s:Skill), (p)-[:HAS_EDUCATION]->(e1:Education), (p)-[:HAS_EDUCATION]->(e2:Education) WHERE toLower(s.name) CONTAINS 'java' AND toLower(s.level) CONTAINS 'expert' AND e1.university <> e2.university RETURN COUNT(DISTINCT p)\n",
    "Question: Where do most candidates get educated?\n",
    "Answer: MATCH (p:Person)-[:HAS_EDUCATION]->(e:Education) RETURN e.university, count(e.university) as alumni ORDER BY alumni DESC LIMIT 1\n",
    "Question: How many people have worked as a Data Scientist in San Francisco?\n",
    "Answer: MATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE toLower(pos.title) CONTAINS 'data scientist' AND toLower(pos.location) CONTAINS 'san francisco' RETURN COUNT(p)\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\",\"question\"], template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "\n",
    "connectionUrl = \"bolt://localhost:7687\"  #input(\"Neo4j Conection URL\")\n",
    "username = \"neo4j\"                          #input(\"DB Username\")\n",
    "password = \"Thireshsidda963i7\" #input(\"DB Password\")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=connectionUrl,\n",
    "    username=username,\n",
    "    password=password\n",
    ")\n",
    "\n",
    "\n",
    "# chain = GraphCypherQAChain.from_llm(\n",
    "#     ChatOpenAI(model_nmae = \"gpt-3.5-turbo \",\n",
    "#                temperature=0,\n",
    "#                max_output_tokens = 2048,), graph=graph, verbose=True,\n",
    "#                cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "#     return_intermediate_steps = True\n",
    "# )\n",
    "\n",
    "\n",
    "# Query the knowledge graph in a RAG application\n",
    "graph.refresh_schema()\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph,\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", openai_api_key=\"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\"),\n",
    "    validate_cypher=True, # Validate relationship directions\n",
    "    verbose=True,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "return_intermediate_steps = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You can run the agent now. Simply provide the command in English. You get Cypher as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_SKILL]->(s:Skill) WHERE toLower(s.name) CONTAINS 'python' RETURN COUNT(DISTINCT p)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'COUNT(DISTINCT p)': 4}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "r = chain(\"\"\"How many Pythonistas are there?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate steps: [{'query': \"MATCH (p:Person)-[:HAS_SKILL]->(s:Skill) WHERE toLower(s.name) CONTAINS 'python' RETURN COUNT(DISTINCT p)\"}, {'context': [{'COUNT(DISTINCT p)': 4}]}]\n",
      "Final answer: There are 4 Pythonistas.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Intermediate steps: {r['intermediate_steps']}\")\n",
    "print(f\"Final answer: {r['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot\n",
    "\n",
    "Time to build a chatbot. We will be using Gradio to quickly try out chatbot that uses a base model. Once chatOpenAI LLM is integrated into Langchain, you will get support for adapter tuned model as well. But here we failed to finetune our base model due to unavaialability of resources for fine tuning. That's why we have used ChatOpenAI from Langchain framework.\n",
    "\n",
    "Running the code below will render a chat widget.You can view the Cypher generated for your input below this rendering.\n",
    "\n",
    "Note - Due to quota limitations, you might see errors while submitting the input. You need to wait a while in between your queries.\n",
    "\n",
    "\n",
    "Some sample questions to try out:\n",
    "\n",
    "1. How many experts do we have on MS Word?\n",
    "2. Who went to most number of universities and how many did they go to?\n",
    "3. Where do most candidates get educated?\n",
    "4. How many people know Delphi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_SKILL]->(s:Skill) WHERE toLower(s.name) CONTAINS 'ms word' AND toLower(s.level) CONTAINS 'expert' RETURN COUNT(p)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'COUNT(p)': 0}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_SKILL]->(s:Skill) WHERE toLower(s.name) CONTAINS 'delphi' RETURN COUNT(p)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'COUNT(p)': 0}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_SKILL]->(s:Skill) WHERE toLower(s.name) CONTAINS 'python' RETURN COUNT(DISTINCT p)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'COUNT(DISTINCT p)': 4}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (p:Person)-[:HAS_EDUCATION]->(e:Education)\n",
      "RETURN e.university, count(e.university) as alumni\n",
      "ORDER BY alumni DESC\n",
      "LIMIT 1\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'e.university': 'American Inter Continental University Atlanta', 'alumni': 2}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-VjUftdDReiqwbZr3sp0I7XxJ on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-VjUftdDReiqwbZr3sp0I7XxJ on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-VjUftdDReiqwbZr3sp0I7XxJ on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key =\"chat_history\", return_messages = True)\n",
    "\n",
    "agent_chain = chain\n",
    "\n",
    "def chat_response(input_text):\n",
    "    response = agent_chain.run(input_text)\n",
    "    return response\n",
    "\n",
    "\n",
    "interface = gr.Interface(fn=chat_response, inputs=\"text\", outputs=\"text\", description=\"Talent Finder Chatbot\")\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
